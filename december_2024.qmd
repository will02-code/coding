---
title: "2024"
format: html
---
# Libraries and theme
```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(data.table)

my_theme <- function() {
  theme_bw() +
    theme(
      plot.title = element_text(size = 22, face = "bold", family = "calibri"),
      strip.text.x = element_text(size = 25, face = "bold", family = "calibri"),
      strip.background.x = element_rect(fill = "white"),
      axis.text.x = element_text(size = 20, colour = "black"),
      axis.text.y = element_text(size = 20, colour = "black"),
      axis.title.x = element_text(size = 22, colour = "black"),
      axis.title.y = element_text(size = 22, colour = "black"),
      legend.title = element_text(size = 22, colour = "black"),
      legend.text = element_text(size = 20, colour = "black"),
      legend.key.size = unit(2, "cm")
    )
}


```


# Load the data
```{r}
# I ended up with a bunch of csvs as output from my processing; this is to load them in and clean everything up
# Probably not relevant for reading in just one file
file_paths_night <- list.files(
  path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\night_quolls_bilbies_blobs",
  pattern = "\\.csv$",
  recursive = TRUE,
  full.names = TRUE
)
file_paths_day <- list.files(
  path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\day_v3",
  pattern = "\\.csv$",
  recursive = TRUE,
  full.names = TRUE
)
file_paths <- c(file_paths_night, file_paths_day)

# Use an anonymous function that adds a column indicating the source file
combined_df <- map_dfr(file_paths, function(path) {
  read_csv(path) %>%
    mutate(source = basename(path)) # Or use the full path
})
# pull out datetime as a datetime object and extract date and time components
combined_df$datetime <- strptime(combined_df$time, format = "%Y:%m:%d %H:%M:%S") # change this to match your datetime format
combined_df$time <- format(combined_df$datetime, "%H:%M:%S") # ditto
combined_df$date <- format(combined_df$datetime, "%Y-%m-%d") # ditto
# probably not relevant for you, but I needed to extract the camera name from the label_path
# also confidence not relevant
cleaned <- combined_df %>%
  mutate(camera = str_extract(label_path, "WCAM\\d{2}")) %>%
  filter(confidence > 0.85 & confidence != "none")

```

# Processing the data
```{r}
# sort by datetime
cleaned <- arrange(cleaned, datetime)
# Initialize time_block column and group_id
cleaned$time_block <- NA_integer_
group_id <- 1
# set the first time_block to 1
start_time <- cleaned$datetime[1]
cleaned$time_block[1] <- group_id
# get list of unique cameras (potentially irrelevant for you)
cameras <- unique(cleaned$camera)
# Create an empty data frame to store results
cleaned_by_camera <- data.frame(
  path_original = character(),
  species = character(),
  confidence = character(),
  bbox = character(),
  path_annotated = character(),
  label_path = character(),
  time = character(),
  source = character(),
  datetime = as.Date(character()),
  date = character(),
  camera = character(),
  time_block = double()
)
# I loop through each camera; you may not need to do this depending on how your data is structured
for (cam in cameras) {
  print(cam)
  # Filter the data for the current camera and sort by datetime
  temp <- cleaned %>%
    arrange(camera) %>%
    filter(camera == cam) %>%
    arrange(datetime)
  # Initialize time_block column for this camera (I have no idea why I did this)
  start_time <- temp$datetime[1]
  temp$time_block[1] <- group_id
  # Loop through each row in the filtered data. All I do in this loop is assign a time_block to each row; one every 10 minutes for each camera. Look at the resulting df for this to make sense. But it's basically so I can process by 10 minute windows
  for (i in 2:nrow(temp)) {
    # set the threshold
    threshold <- 600 # Default threshold of 10 minutes (600 seconds)

    # If current datetime is more than 10 minutes after the start_time,
    # increment the group counter and update the start_time
    if (difftime(temp$datetime[i], start_time, units = "secs") > threshold) {
      group_id <- group_id + 1
      start_time <- temp$datetime[i]
    }

    # Assign the current group_id
    temp$time_block[i] <- group_id
  }
  cleaned_by_camera <- bind_rows(cleaned_by_camera, temp)
}

# Now I summarise the data by time_block and species, getting the count and average confidence for each group
cleaned_by_camera %>%
  group_by(path_original, species) %>% # This group by and mutate gets the number of species in each image (often my AI detects multiple species in one image)
  # You can remove the confidence part
  mutate(
    count = n(),
    confidence = as.numeric(confidence),
    confidence = mean(confidence)
  ) %>%
  ungroup() %>%
  # Okay now I group by time_block and species, and summarise the data. I think I should ALSO group by camera here, but I haven't done for some reason
  # Note that this effectively gets the max number of each species in each time block (and in each camera)
  group_by(time_block, species) %>%
  arrange(desc(count), desc(confidence), .by_group = TRUE) %>%
  summarise(
    confidence = first(confidence), #can skip probs
    datetime = first(datetime),
    path_original = first(path_original),
    camera = first(camera),
    datetime = first(datetime),
    path_annotated = first(path_annotated), # can skip probs
    ai_count = first(count)
  ) %>%
  as.data.table() -> finalised_ai

write.csv(finalised_ai, "coding/december_ai_v3.csv")

```


```{r}

manual <- read.csv(
  "E:\\Wild deserts photos\\2024_04_WCAM_originals\\WTZ_only_Apr01-15_2024_edit.csv"
)
manual_cleaned <- manual %>%
  mutate(
    Date = dmy(Date),
    manual_count = 1
  ) %>%
  # filter(Date >= dmy("03-12-2024")) %>%
  as.data.table()

```

```{r}

manual_cleaned <- manual_cleaned %>%
  mutate(
    Species = case_when(
      Species == "Red Kangaroo" ~ "Kangaroo",
      Species == "Euro" ~ "Kangaroo",
      .default = Species
    )
  ) %>%
  group_by(Species, Date) %>%
  summarise(count = sum(manual_count)) %>%
  group_by(Species) %>%
  arrange(Date) %>%
  mutate(cumulative_count = cumsum(count), method = "manual")
finalised_ai <- finalised_ai %>%
  mutate(
    Date = as.Date(datetime),
    Species = species
  ) %>%
  filter(Date <= dmy("15-04-2024") & Date >= dmy("01-04-2024")) %>%
  group_by(Species, Date) %>%
  summarise(count = sum(ai_count)) %>%
  group_by(Species) %>%
  arrange(Date) %>%
  mutate(cumulative_count = cumsum(count), method = "ai")

combined <- full_join(
  manual_cleaned,
  finalised_ai,
  by = join_by(
    Date,
    Species == Species,
    method == method,
    count == count,
    cumulative_count == cumulative_count
  )
)

```




```{r}
finalised_ai %>%
  ggplot(aes(x = Date, y = cumulative_count)) +
  geom_line(aes(colour = Species)) +
  geom_point(aes(colour = Species)) +
  theme_bw() +
  theme(
    plot.title = element_text(size = 22, face = "bold", family = "calibri"),
    strip.text.x = element_text(size = 25, face = "bold", family = "calibri"),
    strip.background.x = element_rect(fill = "white"),
    axis.text.x = element_text(size = 20, colour = "black"),
    axis.text.y = element_text(size = 20, colour = "black"),
    axis.title.x = element_text(size = 22, colour = "black"),
    axis.title.y = element_text(size = 22, colour = "black")
  )
manual_cleaned %>%
  ggplot(aes(x = Date, y = cumulative_count)) +
  geom_line(aes(colour = Species)) +
  geom_point(aes(colour = Species)) +
  theme_bw() +
  theme(
    plot.title = element_text(size = 22, face = "bold", family = "calibri"),
    strip.text.x = element_text(size = 25, face = "bold", family = "calibri"),
    strip.background.x = element_rect(fill = "white"),
    axis.text.x = element_text(size = 20, colour = "black"),
    axis.text.y = element_text(size = 20, colour = "black"),
    axis.title.x = element_text(size = 22, colour = "black"),
    axis.title.y = element_text(size = 22, colour = "black")
  )


combined %>%
  ggplot(aes(x = Date)) +
  geom_line(aes(y = cumulative_count, linetype = method)) +
  geom_point(aes(y = cumulative_count, shape = method), size = 4) +

  facet_wrap(~Species, scales = "free_y") +
  my_theme() +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))


```










```{r}
time_thresholds <- c(
  "Kangaroo" = 30,
  "Cat" = 300,
  "Unidentifiable" = 60,
  "Rabbit" = 300,
  "Fox" = 300,
  "Quoll" = 300,
  "Bilby" = 300
) # Can be different per species or the same, see comments


confidence_threshold <- 0.85 # Can be different per species and per model (night_quolls_bilbies_blobs and day_v3) #Not relevant for you probably


file_paths_night <- list.files(
  path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\night_quolls_bilbies_blobs",
  pattern = "\\.csv$",
  recursive = TRUE,
  full.names = TRUE
)
file_paths_day <- list.files(
  path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\day_v3",
  pattern = "\\.csv$",
  recursive = TRUE,
  full.names = TRUE
)
file_paths <- c(file_paths_night, file_paths_day)

# Use an anonymous function that adds a column indicating the source file
combined_df <- map_dfr(file_paths, function(path) {
  read_csv(path) %>%
    mutate(source = basename(path)) # Or use the full path
})
combined_df$datetime <- strptime(combined_df$time, format = "%Y:%m:%d %H:%M:%S")
combined_df$time <- format(combined_df$datetime, "%H:%M:%S")
combined_df$date <- format(combined_df$datetime, "%Y-%m-%d")


cleaned <- combined_df %>%
  mutate(camera = str_extract(label_path, "WCAM\\d{2}")) %>%
  filter(confidence > confidence_threshold & confidence != "none")

cameras <- unique(cleaned$camera)
cleaned_by_camera <- data.frame(
  path_original = character(),
  species = character(),
  confidence = character(),
  bbox = character(),
  path_annotated = character(),
  label_path = character(),
  time = character(),
  source = character(),
  datetime = as.Date(character()),
  date = character(),
  camera = character(),
  time_block = double()
)

for (cam in cameras) {
  print(cam)
  temp <- cleaned %>%
    arrange(camera) %>%
    filter(camera == cam) %>%
    arrange(datetime)
  start_time <- temp$datetime[1]
  temp$time_block[1] <- group_id
  for (i in 2:nrow(temp)) {
    current_species <- temp$species[i]
    threshold <- time_thresholds[[current_species]]

    if (is.null(threshold)) {
      threshold <- 60
      cat(paste(
        "Warning: Species '",
        current_species,
        "' not found in time_thresholds. Using default threshold of 60 seconds."
      ))
    }
    # If current datetime is more than 10 minutes after the start_time,
    # increment the group counter and update the start_time
    if (difftime(temp$datetime[i], start_time, units = "secs") > threshold) {
      group_id <- group_id + 1
      start_time <- temp$datetime[i]
    }

    # Assign the current group_id
    temp$time_block[i] <- group_id
  }
  cleaned_by_camera <- bind_rows(cleaned_by_camera, temp)
}
cleaned_by_camera %>%
  group_by(path_original, species) %>%
  mutate(
    count = n(),
    confidence = as.numeric(confidence),
    confidence = mean(confidence)
  ) %>%
  ungroup() %>%
  group_by(time_block, species) %>%
  arrange(desc(count), desc(confidence), .by_group = TRUE) %>%
  summarise(
    confidence = first(confidence),
    datetime = first(datetime),
    path_original = first(path_original),
    camera = first(camera),
    datetime = first(datetime),
    path_annotated = first(path_annotated),
    ai_count = first(count)
  ) %>%
  as.data.table() %>%
  mutate(
    Date = as.Date(datetime),
    Species = species
  ) %>%
  filter(Date <= dmy("15-04-2024") & Date >= dmy("01-04-2024")) %>%
  group_by(Species, Date) %>%
  summarise(count = sum(ai_count)) %>%
  group_by(Species) %>%
  arrange(Date) %>%
  mutate(cumulative_count = cumsum(count), method = "ai") -> finalised_ai

manual <- read.csv(
  "E:\\Wild deserts photos\\2024_04_WCAM_originals\\WTZ_only_Apr01-15_2024_edit.csv"
)
manual_cleaned <- manual %>%
  mutate(
    Date = dmy(Date),
    manual_count = 1
  ) %>%
  # filter(Date >= dmy("03-12-2024")) %>%
  as.data.table() %>%
  mutate(
    Species = case_when(
      Species == "Red Kangaroo" ~ "Kangaroo",
      Species == "Euro" ~ "Kangaroo",
      .default = Species
    )
  ) %>%
  group_by(Species, Date) %>%
  summarise(count = cumsum(manual_count)) %>%
  group_by(Species) %>%
  arrange(Date) %>%
  mutate(cumulative_count = cumsum(count), method = "manual")


```


```{r}
# Define all your species
all_species <- c(
  "Kangaroo",
  "Cat",
  "Unidentifiable",
  "Rabbit",
  "Fox",
  "Quoll",
  "Bilby"
) # Add any others

# Function to calculate the total absolute error for given parameters
evaluate_parameters <- function(params) {
  # params should be a list or vector containing all thresholds
  # Example structure: params$conf_Kangaroo_day, params$conf_Kangaroo_night, params$time_Kangaroo, etc.

  # --- 1. Extract parameters ---
  # It's often easier to pass parameters as a named list or vector
  # Example: Assuming params is a named list like:
  # list(conf_day_Kangaroo=0.8, conf_night_Kangaroo=0.75, time_Kangaroo=30, ...)

  # Dynamically create time_thresholds vector
  time_thresholds <- numeric(length(all_species))
  names(time_thresholds) <- all_species
  for (sp in all_species) {
    time_thresholds[[sp]] <- params[[paste0("time_", sp)]]
  }

  # Confidence thresholds will be used inside the filtering step
  # We need separate thresholds for day and night models, per species

  # --- 2. Read and Process AI Data (Your existing code, adapted) ---
  file_paths_night <- list.files(
    path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\night_quolls_bilbies_blobs",
    pattern = "\\.csv$",
    recursive = TRUE,
    full.names = TRUE
  )
  file_paths_day <- list.files(
    path = "E:\\Wild deserts photos\\2024_04_WCAM_originals_classified\\day_v3",
    pattern = "\\.csv$",
    recursive = TRUE,
    full.names = TRUE
  )
  file_paths <- c(file_paths_night, file_paths_day)

  combined_df <- map_dfr(file_paths, function(path) {
    model_type <- ifelse(grepl("night_", path, fixed = TRUE), "night", "day") # Determine model type
    read_csv(path) %>%
      mutate(source = basename(path), model_type = model_type) # Add model type
  })
  combined_df$datetime <- strptime(
    combined_df$time,
    format = "%Y:%m:%d %H:%M:%S"
  )
  combined_df$time <- format(combined_df$datetime, "%H:%M:%S")
  combined_df$date <- format(combined_df$datetime, "%Y-%m-%d")

  # *** Modification: Apply species- and model-specific confidence thresholds ***
  cleaned <- combined_df %>%
    mutate(camera = str_extract(label_path, "WCAM\\d{2}")) %>%
    filter(confidence != "none") %>% # Keep numeric confidences first
    mutate(confidence = as.numeric(confidence)) %>%
    # Apply dynamic filtering
    filter(
      (model_type == "day" &
        confidence > params[[paste0("conf_day_", species)]]) |
        (model_type == "night" &
          confidence > params[[paste0("conf_night_", species)]])
    )

  # --- 3. Apply Time Buffering (Your existing code) ---
  cameras <- unique(cleaned$camera)
  cleaned_by_camera <- list() # Use a list for efficiency
  group_id_counter <- 1 # Use a counter instead of global variable

  for (cam in cameras) {
    # print(cam) # Optional: remove for speed during optimization
    temp <- cleaned %>%
      filter(camera == cam) %>%
      arrange(datetime)

    if (nrow(temp) == 0) {
      next
    } # Skip if no data for this camera after filtering

    temp$time_block <- NA # Initialize column
    start_time <- temp$datetime[1]
    current_group_id <- group_id_counter
    temp$time_block[1] <- current_group_id

    if (nrow(temp) > 1) {
      for (i in 2:nrow(temp)) {
        current_species <- temp$species[i]
        # Lookup threshold for the current species
        threshold <- time_thresholds[[current_species]]
        # Use a default if species not found (shouldn't happen if all_species is correct)
        if (is.null(threshold)) {
          threshold <- 60
        }

        # Check time difference *within the same species* (common approach)
        # Find the last detection of the *same species* in this camera's temp df
        last_same_species_time <- tail(
          temp$datetime[1:(i - 1)][temp$species[1:(i - 1)] == current_species],
          1
        )

        # If there was no previous detection of this species OR
        # if the time diff from the *last detection of the same species* exceeds threshold
        if (
          length(last_same_species_time) == 0 ||
            difftime(temp$datetime[i], last_same_species_time, units = "secs") >
              threshold
        ) {
          group_id_counter <- group_id_counter + 1
          current_group_id <- group_id_counter
          # start_time <- temp$datetime[i] # Reset start_time? Or just increment group ID?
          # Your original code resets based on *any* previous detection's start_time
          # A common alternative is to check against the *last detection of the same species*
        }
        temp$time_block[i] <- current_group_id # Assign the potentially updated group ID
      }
    }
    cleaned_by_camera[[cam]] <- temp
    group_id_counter <- group_id_counter + 1 # Ensure next camera starts fresh group IDs
  }
  cleaned_by_camera_df <- bind_rows(cleaned_by_camera)

  # --- 4. Summarise AI Counts (Your existing code) ---
  if (nrow(cleaned_by_camera_df) == 0) {
    # Handle case where filtering removed all data
    finalised_ai <- data.frame(
      Species = character(),
      Date = as.Date(character()),
      count = integer(),
      method = character(),
      cumulative_count = integer()
    )
  } else {
    finalised_ai <- cleaned_by_camera_df %>%
      group_by(path_original, species) %>% # Should maybe group by time_block instead? Check logic
      # Your original grouping logic might need adjustment depending on how time_block is assigned
      # Let's assume your original aggregation logic after time_block assignment is correct:
      group_by(time_block, species) %>%
      arrange(desc(confidence), .by_group = TRUE) %>% # Select highest confidence detection per block
      slice(1) %>% # Keep only the first row (highest confidence) per time_block/species
      ungroup() %>%
      mutate(
        Date = as.Date(datetime),
        Species = species,
        ai_count = 1 # Each remaining row is one independent detection event
      ) %>%
      # Filter date range *before* summarising
      filter(Date <= dmy("15-04-2024") & Date >= dmy("01-04-2024")) %>% # Use your specific date range
      group_by(Species, Date) %>%
      summarise(count = n(), .groups = "drop") %>% # Count events per species per day
      # Ensure all species/dates are present for cumsum
      ungroup() %>%
      tidyr::complete(
        Species = all_species,
        Date = seq(dmy("01-04-2024"), dmy("15-04-2024"), by = "day"),
        fill = list(count = 0)
      ) %>%
      filter(Species %in% all_species) %>% # Keep only relevant species
      group_by(Species) %>%
      arrange(Date) %>%
      mutate(cumulative_count = cumsum(count), method = "ai")
  }

  # --- 5. Load and Process Manual Data (Your existing code) ---
  manual <- read.csv(
    "E:\\Wild deserts photos\\2024_04_WCAM_originals\\WTZ_only_Apr01-15_2024_edit.csv"
  )
  manual_cleaned <- manual %>%
    mutate(
      Date = dmy(Date),
      manual_count = 1
    ) %>%
    as.data.table() %>%
    mutate(
      Species = case_when(
        Species == "Red Kangaroo" ~ "Kangaroo",
        Species == "Euro" ~ "Kangaroo",
        .default = Species
      )
    ) %>%
    # Filter date range to match AI
    filter(Date <= dmy("15-04-2024") & Date >= dmy("01-04-2024")) %>%
    # Ensure we only consider species the AI can detect
    filter(Species %in% all_species) %>%
    group_by(Species, Date) %>%
    summarise(count = sum(manual_count), .groups = "drop") %>%
    # Ensure all species/dates are present
    ungroup() %>%
    tidyr::complete(
      Species = all_species,
      Date = seq(dmy("01-04-2024"), dmy("15-04-2024"), by = "day"),
      fill = list(count = 0)
    ) %>%
    filter(Species %in% all_species) %>%
    group_by(Species) %>%
    arrange(Date) %>%
    mutate(cumulative_count = cumsum(count), method = "manual")

  # --- 6. Compare and Calculate Metric ---
  ai_counts <- finalised_ai %>%
    select(Species, Date, ai_count = count)

  manual_counts <- manual_cleaned %>%
    select(Species, Date, manual_count = count)

  comparison_df <- full_join(
    ai_counts,
    manual_counts,
    by = c("Species", "Date")
  ) %>%
    mutate(
      ai_count = ifelse(is.na(ai_count), 0, ai_count),
      manual_count = ifelse(is.na(manual_count), 0, manual_count)
    )

  # Calculate the chosen metric (e.g., total absolute error)
  total_absolute_error <- sum(abs(
    comparison_df$ai_count - comparison_df$manual_count
  ))

  # Return the score (lower is better for error metrics)
  return(total_absolute_error)
}


```



```{r}
# --- Define Species and Parameter Names ---
all_species <- c(
  "Kangaroo",
  "Cat",
  "Unidentifiable",
  "Rabbit",
  "Fox",
  "Quoll",
  "Bilby"
)
param_names <- c(
  paste0("conf_day_", all_species),
  paste0("conf_night_", all_species),
  paste0("time_", all_species)
)


param_ranges <- list(
  # Confidence: Generally > 0.5, maybe < 0.99
  conf_day_min = 0.8,
  conf_day_max = 0.95,
  conf_night_min = 0.8,
  conf_night_max = 0.95, # Night might need lower threshold?
  # Time buffer: e.g., 15 seconds to 10 minutes (600 seconds)
  time_min = 15,
  time_max = 600
)

# --- Randomized Search Loop ---
n_iterations <- 200 # Number of random parameter sets to try
results_list <- list()

for (i in 1:n_iterations) {
  print(paste("Iteration:", i, "/", n_iterations))

  # Sample random parameters
  current_params <- list()
  for (sp in all_species) {
    current_params[[paste0("conf_day_", sp)]] <- runif(
      1,
      param_ranges$conf_day_min,
      param_ranges$conf_day_max
    )
    current_params[[paste0("conf_night_", sp)]] <- runif(
      1,
      param_ranges$conf_night_min,
      param_ranges$conf_night_max
    )
    # Time threshold - often better to sample on log scale or use integers
    current_params[[paste0("time_", sp)]] <- round(runif(
      1,
      param_ranges$time_min,
      param_ranges$time_max
    ))
  }

  # Evaluate this parameter set
  # Use tryCatch to handle potential errors during evaluation
  score <- evaluate_parameters(current_params)

  # Store results
  results_list[[i]] <- list(params = current_params, score = score)

  # Optional: Print progress
  cat("Score:", score, "\n")
}

# --- Find the Best Parameters ---
best_result <- results_list[[which.min(map_dbl(results_list, "score"))]]

print("Best Score Found:")
print(best_result$score)

print("Best Parameters Found:")
# Print neatly
best_params_df <- data.frame(
  parameter = names(best_result$params),
  value = unlist(best_result$params)
)
print(best_params_df)

# You can now use best_result$params as your optimized thresholds
optimised_time_thresholds <- best_params_df[
  grepl("time_", best_params_df$parameter),
]
optimised_conf_thresholds <- best_params_df[
  grepl("conf_", best_params_df$parameter),
]

```