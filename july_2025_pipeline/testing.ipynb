{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1945641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df540a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"folders\": \"E:/wd_camera_test/2025_01_WCAM_originals/outputs\",\n",
    "    \"detection_training_dir\": \"E:/wd_camera_test/2025_01_WCAM_originals/training/detection_training\",\n",
    "    \"classification_training_dir\": \"E:/wd_camera_test/2025_01_WCAM_originals/training/classification_training\",\n",
    "    \"ai_classification_csv\": \"C:/Users/willo/OneDrive - UNSW/Documents/Work/CES/Wild Deserts/Image classification/coding/july_2025_pipeline/config_2025-07-28.csv\",\n",
    "    \"parent_dir\": \"E:/wd_camera_test/\",\n",
    "    \"remote_dir\": \"/home/willwright/Documents/will_drive/wild_deserts_outputs/\",\n",
    "    \"image_width\": 2048,\n",
    "    \"image_height\": 1440,\n",
    "    \"timezone\": \"Australia/Sydney\",\n",
    "    \"species_classes\": [\n",
    "        \"Kangaroo\",\n",
    "        \"Cat\",\n",
    "        \"Rabbit\",\n",
    "        \"Dingo\",\n",
    "        \"Fox\",\n",
    "        \"Bilby\",\n",
    "        \"Quoll\",\n",
    "        \"Unidentifiable\",\n",
    "        \"Bettong\",\n",
    "        \"Crest-tailed mulgara\",\n",
    "        \"Dusky hopping mouse\",\n",
    "        \"Golden bandicoot\",\n",
    "        \"Greater bilby\",\n",
    "        \"Western quoll\",\n",
    "    ],\n",
    "    \"species_regex\": r\"Bilby|blobs|Cat|Dingo|Fox|Kangaroo|Quoll|non_target|Rabbit\",\n",
    "    \"threshold_for_timeblocks\": 600,  # 10 minutes in seconds\n",
    "    \"output_csv_name\": f\"processed_camera_trap_data_{datetime.now().strftime('%Y-%m-%d')}.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcfbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_image_metadata(\n",
    "    images: List[str], tags: List[str] = [\"Categories\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from images using exiftool (required for EXIF Categories).\n",
    "\n",
    "    Args:\n",
    "        images: List of image file paths\n",
    "        tags: EXIF tags to extract (default: [\"Categories\"])\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with image metadata\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing {len(images)} images...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        from exiftool import ExifToolHelper\n",
    "\n",
    "        metadata_list = []\n",
    "\n",
    "        # Process images in batches for better performance\n",
    "        batch_size = 100\n",
    "        total_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "        with ExifToolHelper() as et:\n",
    "            for batch_idx in range(total_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(images))\n",
    "                batch_images = images[start_idx:end_idx]\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Processing batch {batch_idx + 1}/{total_batches} ({len(batch_images)} images)\"\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    # Extract metadata for entire batch\n",
    "                    batch_metadata = et.get_metadata(batch_images)\n",
    "\n",
    "                    for img_path, metadata in zip(batch_images, batch_metadata):\n",
    "                        # Look for Categories in multiple possible EXIF fields\n",
    "                        categories = (\n",
    "                            metadata.get(\"XMP:Categories\", \"\")\n",
    "                            or metadata.get(\"IPTC:Keywords\", \"\")\n",
    "                            or metadata.get(\"XMP:Subject\", \"\")\n",
    "                            or metadata.get(\"EXIF:Categories\", \"\")\n",
    "                            or metadata.get(\"XMP:Categories\", \"\")\n",
    "                            or \"\"\n",
    "                        )\n",
    "\n",
    "                        # If categories is a list, join it\n",
    "                        if isinstance(categories, list):\n",
    "                            categories = \", \".join(categories)\n",
    "\n",
    "                        metadata_list.append(\n",
    "                            {\"SourceFile\": img_path, \"Categories\": str(categories)}\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to extract metadata from batch: {e}\")\n",
    "                    # Add empty entries for failed batch\n",
    "                    for img_path in batch_images:\n",
    "                        metadata_list.append({\"SourceFile\": img_path, \"Categories\": \"\"})\n",
    "\n",
    "        df = pd.DataFrame(metadata_list)\n",
    "\n",
    "    except ImportError:\n",
    "        logger.error(\"ExifTool is required for extracting EXIF Categories tags!\")\n",
    "        logger.error(\"Install with: pip install pyexiftool\")\n",
    "        logger.error(\"And ensure exiftool binary is installed on your system\")\n",
    "        raise ImportError(\n",
    "            \"ExifTool is required for this pipeline - EXIF tags contain essential metadata\"\n",
    "        )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"Metadata extraction completed in {elapsed_time:.2f} seconds\")\n",
    "    logger.info(f\"Extracted Categories from {len(df[df['Categories'] != ''])} images\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_metadata(data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and process image metadata.\n",
    "\n",
    "    Args:\n",
    "        data_df: Raw metadata DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame with additional columns\n",
    "    \"\"\"\n",
    "    df = data_df.copy()\n",
    "\n",
    "    # Clean Categories column (remove \"Categor\" prefix like in R code)\n",
    "    df[\"Categories\"] = df[\"Categories\"].str.replace(\"Categor\", \"\", regex=False)\n",
    "\n",
    "    # Extract correct/incorrect status from Categories tag\n",
    "    df[\"correct_incorrect\"] = df[\"Categories\"].str.extract(\n",
    "        r\"(correct|incorrect)\", expand=False\n",
    "    )\n",
    "\n",
    "    # Extract predicted species from file path (this comes from filename/path structure)\n",
    "    df[\"predicted_species\"] = df[\"SourceFile\"].str.extract(\n",
    "        f\"({CONFIG['species_regex']})\", expand=False\n",
    "    )\n",
    "\n",
    "    # Extract correct species from Categories tag (this is the key difference!)\n",
    "    def get_correct_species(row):\n",
    "        categories = str(row[\"Categories\"]) if pd.notna(row[\"Categories\"]) else \"\"\n",
    "\n",
    "        if row[\"correct_incorrect\"] == \"incorrect\":\n",
    "            # For incorrect predictions, the correct species is in the Categories tag\n",
    "            match = re.search(CONFIG[\"species_regex\"], categories)\n",
    "            return match.group(0) if match else None\n",
    "        elif row[\"correct_incorrect\"] == \"correct\" and \"discard\" not in str(\n",
    "            row[\"SourceFile\"]\n",
    "        ):\n",
    "            # For correct predictions, the predicted species IS the correct species\n",
    "            return row[\"predicted_species\"]\n",
    "        elif \"further_verification\" in str(row[\"SourceFile\"]):\n",
    "            # For further verification, check Categories tag\n",
    "            match = re.search(CONFIG[\"species_regex\"], categories)\n",
    "            return match.group(0) if match else None\n",
    "        elif \"discard\" in str(row[\"SourceFile\"]):\n",
    "            return \"empty\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    df[\"correct_species\"] = df.apply(get_correct_species, axis=1)\n",
    "\n",
    "    # Additional processing columns\n",
    "    df[\"discard\"] = df[\"SourceFile\"].str.contains(\"discard\", na=False)\n",
    "    df[\"confidence\"] = df[\"SourceFile\"].apply(\n",
    "        lambda x: \"low_confidence\" if \"low_confidence\" in str(x) else \"high_confidence\"\n",
    "    )\n",
    "\n",
    "    # Extract camera ID\n",
    "    df[\"camera\"] = df[\"SourceFile\"].str.extract(r\"([WP]CAM\\d{2})\", expand=False)\n",
    "\n",
    "    # Extract and parse datetime\n",
    "    datetime_pattern = r\"(\\d{4}_\\d{2}_\\d{2}_\\d{2}_\\d{2}_\\d{2})\"\n",
    "    df[\"datetime_str\"] = df[\"SourceFile\"].str.extract(datetime_pattern, expand=False)\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[\"datetime_str\"], format=\"%Y_%m_%d_%H_%M_%S\", errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Create ID column\n",
    "    df[\"id\"] = df[\"camera\"].astype(str) + \"_\" + df[\"datetime\"].astype(str)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[\"correct_species\"] = pd.Categorical(df[\"correct_species\"])\n",
    "    df[\"predicted_species\"] = pd.Categorical(df[\"predicted_species\"])\n",
    "\n",
    "    # Log some statistics for verification\n",
    "    logger.info(f\"Processed {len(df)} images:\")\n",
    "    logger.info(f\"  - Correct predictions: {sum(df['correct_incorrect'] == 'correct')}\")\n",
    "    logger.info(\n",
    "        f\"  - Incorrect predictions: {sum(df['correct_incorrect'] == 'incorrect')}\"\n",
    "    )\n",
    "    logger.info(f\"  - Species found: {df['correct_species'].value_counts().to_dict()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_xyxy_to_yolo(\n",
    "    bbox_string: str,\n",
    "    image_width: int = CONFIG[\"image_width\"],\n",
    "    image_height: int = CONFIG[\"image_height\"],\n",
    "    round_digits: int = 4,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from xyxy to YOLO format.\n",
    "\n",
    "    Args:\n",
    "        bbox_string: String representation of bounding box coordinates\n",
    "        image_width: Image width in pixels\n",
    "        image_height: Image height in pixels\n",
    "        round_digits: Number of decimal places to round to\n",
    "\n",
    "    Returns:\n",
    "        YOLO format coordinates as string or None if invalid\n",
    "    \"\"\"\n",
    "    if pd.isna(bbox_string) or bbox_string == \"\":\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Clean and parse coordinates\n",
    "        coords_str = re.sub(r\"[\\[\\]]\", \"\", str(bbox_string))\n",
    "        coords = [float(x.strip()) for x in coords_str.split(\",\")]\n",
    "\n",
    "        if len(coords) != 4:\n",
    "            return None\n",
    "\n",
    "        x_min, y_min, x_max, y_max = coords\n",
    "\n",
    "        # Calculate YOLO format coordinates\n",
    "        box_width = x_max - x_min\n",
    "        box_height = y_max - y_min\n",
    "        x_center_norm = (x_min + box_width / 2) / image_width\n",
    "        y_center_norm = (y_min + box_height / 2) / image_height\n",
    "        width_norm = box_width / image_width\n",
    "        height_norm = box_height / image_height\n",
    "\n",
    "        return \" \".join(\n",
    "            [\n",
    "                str(round(x_center_norm, round_digits)),\n",
    "                str(round(y_center_norm, round_digits)),\n",
    "                str(round(width_norm, round_digits)),\n",
    "                str(round(height_norm, round_digits)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to convert bbox: {bbox_string} - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scale_md_centre(\n",
    "    bbox_string: str,\n",
    "    width: int = CONFIG[\"image_width\"],\n",
    "    height: int = CONFIG[\"image_height\"],\n",
    "    round_digits: int = 3,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Scale MegaDetector center coordinates.\n",
    "\n",
    "    Args:\n",
    "        bbox_string: String representation of bounding box coordinates\n",
    "        width: Image width\n",
    "        height: Image height\n",
    "        round_digits: Number of decimal places to round to\n",
    "\n",
    "    Returns:\n",
    "        Scaled coordinates as string or None if invalid\n",
    "    \"\"\"\n",
    "    if pd.isna(bbox_string) or bbox_string == \"\":\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Clean and parse coordinates\n",
    "        coords_str = re.sub(r\"[\\[\\]]\", \"\", str(bbox_string))\n",
    "        coords = [float(x.strip()) for x in coords_str.split(\",\")]\n",
    "\n",
    "        if len(coords) != 4:\n",
    "            return None\n",
    "\n",
    "        x_min, y_min, w, h = coords\n",
    "\n",
    "        x_c = x_min + w / 2\n",
    "        y_c = y_min + h / 2\n",
    "\n",
    "        return \" \".join(\n",
    "            [\n",
    "                str(round(x_c, round_digits)),\n",
    "                str(round(y_c, round_digits)),\n",
    "                str(round(w, round_digits)),\n",
    "                str(round(h, round_digits)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to scale bbox: {bbox_string} - {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def safe_file_copy(\n",
    "    df: pd.DataFrame, source_col: str, dest_col: str, file_type: str = \"file\"\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Safely copy files with error handling.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with source and destination paths\n",
    "        source_col: Column name for source paths\n",
    "        dest_col: Column name for destination paths\n",
    "        file_type: Type of file being copied (for logging)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with success and error counts\n",
    "    \"\"\"\n",
    "    logger.info(f\"Copying {len(df)} {file_type}s...\")\n",
    "\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Copying {file_type}s\"):\n",
    "        source = row[source_col]\n",
    "        dest = row[dest_col]\n",
    "\n",
    "        # Create destination directory\n",
    "        Path(dest).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            if Path(source).exists():\n",
    "                shutil.copy2(source, dest)\n",
    "                success_count += 1\n",
    "                if (i + 1) % 50 == 0 or (i + 1) == len(df):\n",
    "                    logger.info(f\"Progress: {i + 1}/{len(df)} files copied\")\n",
    "            else:\n",
    "                logger.warning(f\"⚠ Source {file_type} not found: {source}\")\n",
    "                error_count += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Failed to copy {file_type} {Path(source).name}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    logger.info(\n",
    "        f\"✓ {file_type} copy complete: {success_count} successful, {error_count} errors\"\n",
    "    )\n",
    "    return {\"success\": success_count, \"errors\": error_count}\n",
    "\n",
    "\n",
    "def write_yolo_labels(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Write YOLO labels to files.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with labels and destination paths\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with success and error counts\n",
    "    \"\"\"\n",
    "    logger.info(f\"Writing {len(df)} YOLO label files...\")\n",
    "\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Writing labels\"):\n",
    "        label = row[\"label\"]\n",
    "        dest_path = row[\"destination_labels\"]\n",
    "\n",
    "        try:\n",
    "            Path(dest_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(dest_path, \"w\") as f:\n",
    "                f.write(str(label))\n",
    "            success_count += 1\n",
    "            if (i + 1) % 50 == 0 or (i + 1) == len(df):\n",
    "                logger.info(f\"Progress: {i + 1}/{len(df)} labels written\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Failed to create label {Path(dest_path).name}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    logger.info(\n",
    "        f\"✓ Label writing complete: {success_count} successful, {error_count} errors\"\n",
    "    )\n",
    "    return {\"success\": success_count, \"errors\": error_count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_night_day(row):\n",
    "                if \"night\" in str(row[\"SourceFile\"]):\n",
    "                    return \"night\"\n",
    "                elif \"day\" in str(row[\"SourceFile\"]):\n",
    "                    return \"day\"\n",
    "                elif \"model\" in row and row[\"model\"] == \"day\":\n",
    "                    return \"day\"\n",
    "                elif \"model\" in row and row[\"model\"] == \"night\":\n",
    "                    return \"night\"\n",
    "                else:\n",
    "                    return \"day\"\n",
    "\n",
    "def process_camera_trap_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to process camera trap images.\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame with camera trap data\n",
    "    \"\"\"\n",
    "    # 1. Load and process image metadata\n",
    "    logger.info(\"Step 1: Loading image metadata...\")\n",
    "\n",
    "    images = glob.glob(os.path.join(CONFIG[\"folders\"],\"**\" , \"*.JPG\"), recursive=True)\n",
    "\n",
    "    if not images:\n",
    "        raise ValueError(\"No images found in specified folders\")\n",
    "\n",
    "    logger.info(f\"Found {len(images)} images\")\n",
    "\n",
    "    # Extract metadata\n",
    "    raw_metadata = extract_image_metadata(images)\n",
    "\n",
    "    # Process metadata\n",
    "    final_df = process_metadata(raw_metadata)\n",
    "\n",
    "    # 2. Process detection training data\n",
    "    logger.info(\"\\nStep 2: Processing detection training data...\")\n",
    "    process_detection_training(final_df)\n",
    "\n",
    "    # 3. Process classification training data\n",
    "    logger.info(\"\\nStep 3: Processing classification training data...\")\n",
    "    process_classification_training(final_df)\n",
    "\n",
    "    logger.info(\"\\n✅ Pipeline completed successfully!\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_detection_training(final_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Process detection training data.\n",
    "\n",
    "    Args:\n",
    "        final_df: Processed metadata DataFrame\n",
    "    \"\"\"\n",
    "    # Process incorrect bounding boxes\n",
    "    logger.info(\"Processing incorrect bounding boxes...\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Process all labeled detections\n",
    "    logger.info(\"Processing all labeled detections...\")\n",
    "\n",
    "    # Load AI classification data\n",
    "    if not Path(CONFIG[\"ai_classification_csv\"]).exists():\n",
    "        logger.warning(\n",
    "            f\"AI classification CSV not found: {CONFIG['ai_classification_csv']}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        ai_classification_df = pd.read_csv(CONFIG[\"ai_classification_csv\"])\n",
    "        ai_classification_df[\"filename\"] = ai_classification_df[\"blank_path\"].apply(\n",
    "            lambda x: Path(x).name\n",
    "        )\n",
    "        ai_classification_df[\"datetime\"] = pd.to_datetime(ai_classification_df[\"time\"])\n",
    "        ai_classification_df[\"id\"] = (\n",
    "            ai_classification_df[\"camera\"].astype(str)\n",
    "            + \"_\"\n",
    "            + ai_classification_df[\"datetime\"].astype(str)\n",
    "        )\n",
    "        incorrect_bounding = final_df[\n",
    "        (final_df[\"correct_incorrect\"] == \"incorrect\")\n",
    "        & (final_df[\"correct_species\"] == \"non_target\")\n",
    "        ].copy()\n",
    "        if len(incorrect_bounding) > 0:\n",
    "\n",
    "            incorrect_bounding = incorrect_bounding.merge(\n",
    "            ai_classification_df, on=\"id\", how=\"left\"\n",
    "            )\n",
    "            incorrect_bounding = incorrect_bounding[\n",
    "                incorrect_bounding[\"correct_species\"] == \"non_target\"\n",
    "            ].copy()\n",
    "            incorrect_bounding[\"blank_path\"] = incorrect_bounding[\n",
    "                \"blank_path\"\n",
    "            ].str.replace(CONFIG[\"remote_dir\"], CONFIG[\"parent_dir\"], regex=False)\n",
    "            incorrect_bounding[\"detection_image_path\"] = incorrect_bounding[\n",
    "                \"blank_path\"\n",
    "            ]\n",
    "            incorrect_bounding[\"detection_label_path\"] = (\n",
    "                incorrect_bounding[\"detection_image_path\"]\n",
    "                .str.replace(\"images\", \"labels\", regex=False)\n",
    "                .str.replace(\".jpg\", \".txt\", regex=False)\n",
    "            )\n",
    "            incorrect_bounding[\"night_day\"] = incorrect_bounding.apply(\n",
    "                get_night_day, axis=1\n",
    "            )\n",
    "            incorrect_bounding[\"destination_images\"] = (\n",
    "                CONFIG[\"detection_training_dir\"]\n",
    "                + \"/\"\n",
    "                + incorrect_bounding[\"night_day\"]\n",
    "                + \"/images/\"\n",
    "                + incorrect_bounding[\"detection_image_path\"].apply(lambda x: Path(x).name)\n",
    "            )\n",
    "            incorrect_bounding[\"destination_labels\"] = (\n",
    "                incorrect_bounding[\"destination_images\"]\n",
    "                .str.replace(\"images\", \"labels\", regex=False)\n",
    "                .str.replace(\".jpg\", \".txt\", regex=False)\n",
    "            \n",
    "            )\n",
    "\n",
    "            results = safe_file_copy(\n",
    "                incorrect_bounding,\n",
    "                \"detection_image_path\",\n",
    "                \"destination_images\",\n",
    "                \"detection image\",\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Processed {results['success']} incorrect bounding images ({results['errors']} errors)\"\n",
    "            )       \n",
    "        # Join with final_df\n",
    "        all_labelled_detections = final_df.merge(\n",
    "            ai_classification_df, on=\"id\", how=\"left\"\n",
    "        )\n",
    "        all_labelled_detections = all_labelled_detections[\n",
    "            all_labelled_detections[\"correct_species\"] != \"non_target\"\n",
    "        ].copy()\n",
    "\n",
    "        if len(all_labelled_detections) > 0:\n",
    "            # Process paths and labels\n",
    "            all_labelled_detections[\"blank_path\"] = all_labelled_detections[\n",
    "                \"blank_path\"\n",
    "            ].str.replace(CONFIG[\"remote_dir\"], CONFIG[\"parent_dir\"], regex=False)\n",
    "            all_labelled_detections[\"detection_image_path\"] = all_labelled_detections[\n",
    "                \"blank_path\"\n",
    "            ]\n",
    "\n",
    "            # Determine night/day\n",
    "            \n",
    "\n",
    "            all_labelled_detections[\"night_day\"] = all_labelled_detections.apply(\n",
    "                get_night_day, axis=1\n",
    "            )\n",
    "\n",
    "            all_labelled_detections[\"destination_images\"] = (\n",
    "                CONFIG[\"detection_training_dir\"]\n",
    "                + \"/\"\n",
    "                + all_labelled_detections[\"night_day\"]\n",
    "                + \"/images/\"\n",
    "                + all_labelled_detections[\"detection_image_path\"].apply(\n",
    "                    lambda x: Path(x).name if pd.notna(x) else \"\"\n",
    "                )\n",
    "            )\n",
    "            all_labelled_detections[\"destination_labels\"] = (\n",
    "                all_labelled_detections[\"destination_images\"]\n",
    "                .str.replace(\"images\", \"labels\", regex=False)\n",
    "                .str.replace(\".JPG\", \".txt\", regex=False)\n",
    "            )\n",
    "\n",
    "            # Create labels\n",
    "            def create_label(row):\n",
    "                if row[\"correct_species\"] == \"empty\":\n",
    "                    return None\n",
    "\n",
    "                try:\n",
    "                    species_idx = CONFIG[\"species_classes\"].index(\n",
    "                        row[\"correct_species\"]\n",
    "                    )\n",
    "                except (ValueError, TypeError):\n",
    "                    return None\n",
    "\n",
    "                if \"species\" in row and row[\"species\"] == \"animal\":\n",
    "                    bbox_coords = scale_md_centre(row.get(\"bbox\", \"\"))\n",
    "                else:\n",
    "                    bbox_coords = convert_xyxy_to_yolo(row.get(\"bbox\", \"\"))\n",
    "\n",
    "                if bbox_coords:\n",
    "                    return f\"{species_idx} {bbox_coords}\"\n",
    "                return None\n",
    "\n",
    "            all_labelled_detections[\"label\"] = all_labelled_detections.apply(\n",
    "                create_label, axis=1\n",
    "            )\n",
    "\n",
    "            # Filter valid labels and group by ID\n",
    "            valid_detections = all_labelled_detections[\n",
    "                all_labelled_detections[\"label\"].notna()\n",
    "                & all_labelled_detections[\"detection_image_path\"].notna()\n",
    "            ].copy()\n",
    "\n",
    "            if len(valid_detections) > 0:\n",
    "                # Group by ID and combine labels\n",
    "                grouped = (\n",
    "                    valid_detections.groupby(\"id\")\n",
    "                    .agg(\n",
    "                        {\n",
    "                            \"detection_image_path\": \"first\",\n",
    "                            \"destination_images\": \"first\",\n",
    "                            \"destination_labels\": \"first\",\n",
    "                            \"label\": lambda x: \"\\n\".join(x.dropna().unique()),\n",
    "                        }\n",
    "                    )\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                # Write labels\n",
    "                label_results = write_yolo_labels(grouped)\n",
    "\n",
    "                # Copy images\n",
    "                image_results = safe_file_copy(\n",
    "                    grouped,\n",
    "                    \"detection_image_path\",\n",
    "                    \"destination_images\",\n",
    "                    \"detection image\",\n",
    "                )\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Processed {len(grouped)} labeled detections: \"\n",
    "                    f\"{image_results['success']} images, {label_results['success']} labels\"\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing labeled detections: {e}\")\n",
    "\n",
    "\n",
    "def process_classification_training(final_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Process classification training data.\n",
    "\n",
    "    Args:\n",
    "        final_df: Processed metadata DataFrame\n",
    "    \"\"\"\n",
    "    classification_data = final_df[final_df[\"correct_species\"] != \"non_target\"].copy()\n",
    "\n",
    "    classification_data[\"current_image_path\"] = classification_data[\"SourceFile\"]\n",
    "\n",
    "    def get_night_day_class(path):\n",
    "        if \"night\" in str(path):\n",
    "            return \"night\"\n",
    "        elif \"day\" in str(path):\n",
    "            return \"day\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    classification_data[\"night_day\"] = classification_data[\"current_image_path\"].apply(\n",
    "        get_night_day_class\n",
    "    )\n",
    "\n",
    "    classification_data[\"destination_images\"] = (\n",
    "        CONFIG[\"classification_training_dir\"]\n",
    "        + \"/\"\n",
    "        + classification_data[\"night_day\"]\n",
    "        + \"/\"\n",
    "        + classification_data[\"correct_species\"].astype(str)\n",
    "        + \"/\"\n",
    "        + classification_data[\"correct_incorrect\"].astype(str)\n",
    "        + \"_\"\n",
    "        + classification_data[\"current_image_path\"].apply(lambda x: Path(x).name)\n",
    "    )\n",
    "\n",
    "    # Filter valid entries\n",
    "    valid_data = classification_data[\n",
    "        classification_data[\"current_image_path\"].notna()\n",
    "        & classification_data[\"destination_images\"].notna()\n",
    "        & classification_data[\"night_day\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "    if len(valid_data) > 0:\n",
    "        results = safe_file_copy(\n",
    "            valid_data,\n",
    "            \"current_image_path\",\n",
    "            \"destination_images\",\n",
    "            \"classification image\",\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Processed {results['success']} classification images ({results['errors']} errors)\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84878756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 13:29:09,198 - INFO - Step 1: Loading image metadata...\n",
      "2025-07-30 13:29:09,205 - INFO - Found 810 images\n",
      "2025-07-30 13:29:09,206 - INFO - Processing 810 images...\n",
      "2025-07-30 13:29:09,488 - INFO - Processing batch 1/9 (100 images)\n",
      "2025-07-30 13:29:10,032 - INFO - Processing batch 2/9 (100 images)\n",
      "2025-07-30 13:29:10,539 - INFO - Processing batch 3/9 (100 images)\n",
      "2025-07-30 13:29:11,049 - INFO - Processing batch 4/9 (100 images)\n",
      "2025-07-30 13:29:11,488 - INFO - Processing batch 5/9 (100 images)\n",
      "2025-07-30 13:29:11,905 - INFO - Processing batch 6/9 (100 images)\n",
      "2025-07-30 13:29:12,345 - INFO - Processing batch 7/9 (100 images)\n",
      "2025-07-30 13:29:12,771 - INFO - Processing batch 8/9 (100 images)\n",
      "2025-07-30 13:29:13,370 - INFO - Processing batch 9/9 (10 images)\n",
      "2025-07-30 13:29:13,472 - INFO - Metadata extraction completed in 4.27 seconds\n",
      "2025-07-30 13:29:13,472 - INFO - Extracted Categories from 810 images\n",
      "2025-07-30 13:29:13,512 - INFO - Processed 810 images:\n",
      "2025-07-30 13:29:13,513 - INFO -   - Correct predictions: 724\n",
      "2025-07-30 13:29:13,515 - INFO -   - Incorrect predictions: 12\n",
      "2025-07-30 13:29:13,517 - INFO -   - Species found: {'Kangaroo': 504, 'empty': 200, 'non_target': 61, 'Rabbit': 33, 'Cat': 9, 'Bilby': 3}\n",
      "2025-07-30 13:29:13,517 - INFO - \n",
      "Step 2: Processing detection training data...\n",
      "2025-07-30 13:29:13,517 - INFO - Processing incorrect bounding boxes...\n",
      "2025-07-30 13:29:13,517 - INFO - Processing all labeled detections...\n",
      "<positron-console-cell-29>:75: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "2025-07-30 13:29:13,543 - ERROR - Error processing labeled detections: Unknown datetime string format, unable to parse: 2025_01_23_23_05_30, at position 0\n",
      "2025-07-30 13:29:13,545 - INFO - \n",
      "Step 3: Processing classification training data...\n",
      "2025-07-30 13:29:13,562 - INFO - Copying 531 classification images...\n",
      "Copying classification images:   2%|▏         | 8/531 [00:00<00:26, 20.02it/s]2025-07-30 13:29:14,141 - INFO - Progress: 50/531 files copied\n",
      "Copying classification images:   9%|▉         | 50/531 [00:00<00:03, 127.43it/s]2025-07-30 13:29:14,255 - INFO - Progress: 100/531 files copied\n",
      "Copying classification images:  27%|██▋       | 142/531 [00:00<00:01, 286.30it/s]2025-07-30 13:29:14,366 - INFO - Progress: 150/531 files copied\n",
      "Copying classification images:  36%|███▌      | 191/531 [00:00<00:00, 343.67it/s]2025-07-30 13:29:14,468 - INFO - Progress: 200/531 files copied\n",
      "Copying classification images:  44%|████▎     | 232/531 [00:01<00:00, 313.06it/s]2025-07-30 13:29:14,635 - INFO - Progress: 250/531 files copied\n",
      "Copying classification images:  54%|█████▎    | 285/531 [00:01<00:00, 370.19it/s]2025-07-30 13:29:14,774 - INFO - Progress: 300/531 files copied\n",
      "Copying classification images:  62%|██████▏   | 327/531 [00:01<00:00, 364.26it/s]2025-07-30 13:29:14,874 - INFO - Progress: 350/531 files copied\n",
      "Copying classification images:  70%|███████   | 373/531 [00:01<00:00, 389.55it/s]2025-07-30 13:29:14,991 - INFO - Progress: 400/531 files copied\n",
      "Copying classification images:  79%|███████▊  | 418/531 [00:01<00:00, 406.20it/s]2025-07-30 13:29:15,091 - INFO - Progress: 450/531 files copied\n",
      "Copying classification images:  87%|████████▋ | 461/531 [00:01<00:00, 369.20it/s]2025-07-30 13:29:15,251 - INFO - Progress: 500/531 files copied\n",
      "Copying classification images:  95%|█████████▌| 506/531 [00:01<00:00, 389.09it/s]2025-07-30 13:29:15,317 - INFO - Progress: 531/531 files copied\n",
      "Copying classification images: 100%|██████████| 531/531 [00:01<00:00, 302.37it/s]\n",
      "2025-07-30 13:29:15,322 - INFO - ✓ classification image copy complete: 531 successful, 0 errors\n",
      "2025-07-30 13:29:15,322 - INFO - Processed 531 classification images (0 errors)\n",
      "2025-07-30 13:29:15,322 - INFO - \n",
      "✅ Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "final_results = process_camera_trap_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c2f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/wd_camera_test/2025_01_WCAM_originals/outputs\\\\*.JPEG'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.path.join(CONFIG[\"folders\"], pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b816427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 12:56:49,141 - INFO - Processing 810 images...\n",
      "2025-07-30 12:56:49,541 - INFO - Processing batch 1/9 (100 images)\n",
      "2025-07-30 12:56:50,168 - INFO - Processing batch 2/9 (100 images)\n",
      "2025-07-30 12:56:50,804 - INFO - Processing batch 3/9 (100 images)\n",
      "2025-07-30 12:56:51,674 - INFO - Processing batch 4/9 (100 images)\n",
      "2025-07-30 12:56:52,322 - INFO - Processing batch 5/9 (100 images)\n",
      "2025-07-30 12:56:52,927 - INFO - Processing batch 6/9 (100 images)\n",
      "2025-07-30 12:56:53,581 - INFO - Processing batch 7/9 (100 images)\n",
      "2025-07-30 12:56:54,191 - INFO - Processing batch 8/9 (100 images)\n",
      "2025-07-30 12:56:54,954 - INFO - Processing batch 9/9 (10 images)\n",
      "2025-07-30 12:56:55,095 - INFO - Metadata extraction completed in 5.95 seconds\n",
      "2025-07-30 12:56:55,095 - INFO - Extracted Categories from 810 images\n",
      "2025-07-30 12:56:55,157 - INFO - Processed 810 images:\n",
      "2025-07-30 12:56:55,167 - INFO -   - Correct predictions: 724\n",
      "2025-07-30 12:56:55,169 - INFO -   - Incorrect predictions: 12\n",
      "2025-07-30 12:56:55,171 - INFO -   - Species found: {'Kangaroo': 504, 'empty': 200, 'non_target': 61, 'Rabbit': 33, 'Cat': 9, 'Bilby': 3}\n"
     ]
    }
   ],
   "source": [
    "images = glob.glob(os.path.join(CONFIG[\"folders\"],\"**\" , \"*.JPG\"), recursive=True)\n",
    "raw_metadata = extract_image_metadata(images)\n",
    "\n",
    "# Process metadata\n",
    "final_df = process_metadata(raw_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc2fa3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 12:58:34,219 - INFO - Copying 5 detection images...\n",
      "Copying detection images: 100%|██████████| 5/5 [00:00<00:00, 14.06it/s]\n",
      "2025-07-30 12:58:34,574 - INFO - ✓ detection image copy complete: 5 successful, 0 errors\n",
      "2025-07-30 12:58:34,575 - INFO - Processed 5 incorrect bounding images (0 errors)\n"
     ]
    }
   ],
   "source": [
    "incorrect_bounding = final_df[\n",
    "        (final_df[\"correct_incorrect\"] == \"incorrect\")\n",
    "        & (final_df[\"correct_species\"] == \"non_target\")\n",
    "    ].copy()\n",
    "\n",
    "if len(incorrect_bounding) > 0:\n",
    "    incorrect_bounding[\"detection_image_path\"] = incorrect_bounding[\"SourceFile\"]\n",
    "    incorrect_bounding[\"detection_label_path\"] = (\n",
    "        incorrect_bounding[\"detection_image_path\"]\n",
    "        .str.replace(\"images\", \"labels\", regex=False)\n",
    "        .str.replace(\".jpg\", \".txt\", regex=False)\n",
    "    )\n",
    "    incorrect_bounding[\"night_day\"] = incorrect_bounding[\"SourceFile\"].apply(\n",
    "        lambda x: \"night\" if \"night\" in str(x) else \"day\"\n",
    "    )\n",
    "    incorrect_bounding[\"destination_images\"] = (\n",
    "        CONFIG[\"detection_training_dir\"]\n",
    "        + \"/\"\n",
    "        + incorrect_bounding[\"night_day\"]\n",
    "        + \"/images/\"\n",
    "        + incorrect_bounding[\"detection_image_path\"].apply(lambda x: Path(x).name)\n",
    "    )\n",
    "    incorrect_bounding[\"destination_labels\"] = (\n",
    "        CONFIG[\"detection_training_dir\"]\n",
    "        + \"/\"\n",
    "        + incorrect_bounding[\"night_day\"]\n",
    "        + \"/labels/\"\n",
    "        + incorrect_bounding[\"detection_label_path\"].apply(lambda x: Path(x).name)\n",
    "    )\n",
    "\n",
    "    results = safe_file_copy(\n",
    "        incorrect_bounding,\n",
    "        \"detection_image_path\",\n",
    "        \"destination_images\",\n",
    "        \"detection image\",\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Processed {results['success']} incorrect bounding images ({results['errors']} errors)\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
